{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1O1B7nC-9P7tUPuWyc3p4qc8nBKlJI_Aj",
      "authorship_tag": "ABX9TyPXhIYXHn55e5FZOc76lO24",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seabisilas-alt/Github-Work/blob/main/rq1_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "9055c10d"
      },
      "source": [
        "# Uninstall and reinstall numpy, pandas, gensim and fasttext\n",
        "!pip uninstall numpy pandas fasttext -y\n",
        "!pip install numpy pandas==2.2.2 fasttext tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_ZgURtMpA83"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoModel\n",
        "import fasttext\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, LSTM, RepeatVector, Dense, Lambda, Layer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "KAQyeQtHrZEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Domains = pd.read_csv('/content/drive/MyDrive/MSC Work/RQ1/Dataset/All Domains.csv')"
      ],
      "metadata": {
        "id": "cshrcbRErN0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Domains"
      ],
      "metadata": {
        "id": "1LKg9bYxrhsS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Legitimate_Domains_df = pd.read_csv('/content/drive/MyDrive/MSC Work/RQ1/Dataset/Domains/Benign Domains.csv', header=None, names=['Legitimate Domain'])"
      ],
      "metadata": {
        "id": "Hn4GAmTLy1MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Legitimate_Domains_df"
      ],
      "metadata": {
        "id": "RnxnT9d_y3SO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase\n",
        "Legitimate_Domains_df['Legitimate Domain'] = Legitimate_Domains_df['Legitimate Domain'].str.lower()\n",
        "\n",
        "# Split\n",
        "train_df, test_df = train_test_split(Legitimate_Domains_df, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "fDHnqtGF0I62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Illegitimate_Domains_df = pd.read_csv('/content/drive/MyDrive/MSC Work/RQ1/Dataset/Labelled Domains.csv')"
      ],
      "metadata": {
        "id": "joluUjVt6SCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Illegitimate_Domains_df"
      ],
      "metadata": {
        "id": "edLK8kXa7HEJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Combined = pd.concat([Illegitimate_Domains_df, Legitimate_Domains_df])"
      ],
      "metadata": {
        "id": "GlU2DQErw2dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Combined['Domains'] = Combined['Domains'].fillna(Legitimate_Domains_df['Legitimate Domain'])"
      ],
      "metadata": {
        "id": "Y8W4zjglw8si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Combined['Label'] = Combined['Label'].replace({'Malicious':'__label__Malicious'})"
      ],
      "metadata": {
        "id": "z4wJenmHxKKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Combined['Label'] = Combined['Label'].fillna('__label__Legit')"
      ],
      "metadata": {
        "id": "BlhcLcH3-lT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Combined"
      ],
      "metadata": {
        "id": "mTh-8RXkyRyf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Combined['Domains'] = Combined['Domains'].str.lower()\n"
      ],
      "metadata": {
        "id": "4CI0tEw5ydqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Training_df, Testing_df = train_test_split(Combined[['Domains', 'Label']], test_size=0.2, random_state=42, stratify=Combined['Label'])"
      ],
      "metadata": {
        "id": "E5SjADW-yxpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Training_df['Label'].value_counts()"
      ],
      "metadata": {
        "id": "nhzd-bvx1ZBO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FastText Embeddings"
      ],
      "metadata": {
        "id": "usyNi54eDQGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Training_df['ft'] = Training_df['Label'] + ' ' + Training_df['Domains']\n",
        "\n",
        "# Save to text files\n",
        "Training_df['ft'].to_csv(\"/content/drive/MyDrive/MSC Work/RQ1/Dataset/Domains/fasttext_train.txt\", index=False, header=False)"
      ],
      "metadata": {
        "id": "nBQDyMt527aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.train_supervised(\n",
        "    input=\"/content/drive/MyDrive/MSC Work/RQ1/Dataset/Domains/fasttext_train.txt\",\n",
        "    dim=100,\n",
        "    minn=2, maxn=5,      # use character n-grams (good for domains)\n",
        "    lr=0.1,\n",
        "    epoch=30,\n",
        "    label=\"__label__\",\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "id": "me4F5xfd47T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_model(\"/content/drive/MyDrive/MSC Work/RQ1/FastText_Model/domain_classifier[100].bin\")"
      ],
      "metadata": {
        "id": "lsxtgaUQ_YSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastText_model = fasttext.load_model(\"/content/drive/MyDrive/MSC Work/RQ1/FastText_Model/domain_classifier[100].bin\")"
      ],
      "metadata": {
        "id": "_UCaa6BTRDf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract domains + labels from test set\n",
        "test_domains = Testing_df['Domains'].tolist()\n",
        "test_labels = Testing_df['Label'].tolist()"
      ],
      "metadata": {
        "id": "yVmO77OiPWLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_FastText_emebddings(domains, model):\n",
        "    embeddings = []\n",
        "    for domain in domains:\n",
        "        embedding = model.get_word_vector(domain)\n",
        "        embeddings.append(embedding)\n",
        "    return np.array(embeddings)"
      ],
      "metadata": {
        "id": "TKtOkj6vSp-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get embeddings (vector representations) from FastText\n",
        "vectors_embeddings = get_FastText_emebddings(test_domains, FastText_model)"
      ],
      "metadata": {
        "id": "L-XtEL6B732g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors_embeddings"
      ],
      "metadata": {
        "id": "-XzEnLAw746I",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce to 2D\n",
        "pca = PCA(n_components=2)\n",
        "PCA_Embeddings = pca.fit_transform(vectors_embeddings)\n"
      ],
      "metadata": {
        "id": "3GMuL3bgPB0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_labels = [l.replace(\"__label__\", \"\") for l in test_labels]"
      ],
      "metadata": {
        "id": "SnP_dgFePasa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced"
      ],
      "metadata": {
        "id": "24xJcGN3PDf8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "plt.figure(figsize=(10,7))\n",
        "\n",
        "for label, color in [('Legit', 'green'), ('Malicious', 'red')]:\n",
        "    idx = [i for i, l in enumerate(clean_labels) if l == label]\n",
        "    plt.scatter(PCA_Embeddings[idx,0], PCA_Embeddings[idx,1], label=label, alpha=0.6, s=15, c=color)\n",
        "\n",
        "plt.title(\"FastText Domain Embeddings (PCA 2D)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vMz8Oy815O0N",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run t-SNE\n",
        "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, random_state=42)\n",
        "reduced = tsne.fit_transform(vectors_embeddings)  # reduced.shape -> (n_samples, 2)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,6))\n",
        "for label, color in [('Legit', 'green'), ('Malicious', 'red')]:\n",
        "    idx = [i for i, l in enumerate(clean_labels) if l == label]\n",
        "    plt.scatter(reduced[idx,0], reduced[idx,1], label=label, alpha=0.6, s=20, c=color)\n",
        "\n",
        "plt.xlabel(\"t-SNE Component 1\")\n",
        "plt.ylabel(\"t-SNE Component 2\")\n",
        "plt.title(\"t-SNE of Domain Embeddings\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_SW1sc3qQ27N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = 2\n",
        "\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(vectors_embeddings)"
      ],
      "metadata": {
        "id": "lkbiYBbmTvkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_labels"
      ],
      "metadata": {
        "id": "_t6QNEPMsI2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Testing_df"
      ],
      "metadata": {
        "id": "9MG-3R7WsWDh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Testing_df['Label'] = Testing_df['Label'].replace({'__label__Malicious' : 1, '__label__Legit' : 0})"
      ],
      "metadata": {
        "id": "_djhYeZCpYXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Testing_df['Cluster'] = cluster_labels"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Qr0oJzmNojKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(Testing_df['Label'], Testing_df['Cluster'])\n",
        "\n",
        "# Create heatmap\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "            xticklabels=[\"Legit\", \"Malicious\"],\n",
        "            yticklabels=Testing_df['Label'].unique())\n",
        "\n",
        "plt.xlabel(\"Predicted Cluster\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix: KMeans Clustering vs True Labels\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "a-kiYY2xpCrc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(Testing_df['Label'], Testing_df['Cluster']))"
      ],
      "metadata": {
        "id": "NGcE4C8BqUbw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot clusters\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.scatter(reduced[:,0], reduced[:,1], c=cluster_labels, cmap=\"coolwarm\", s=15, alpha=0.6)\n",
        "\n",
        "# Optional: annotate with domain names (careful if you have too many)\n",
        "# for i, domain in enumerate(test_domains[:100]):  # annotate first 100 only\n",
        "#     plt.annotate(domain, (reduced[i,0], reduced[i,1]), fontsize=7, alpha=0.7)\n",
        "\n",
        "plt.title(\"KMeans Clusters on FastText Domain Embeddings (PCA 2D)\")\n",
        "plt.colorbar(label=\"Cluster\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4r4txmCXnx9u",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KMeans is perfectly precise for Legit domains → if it says a domain is Legit, you can trust it.\n",
        "\n",
        "But it misses some Legit domains (lower recall).\n",
        "\n",
        "For Malicious domains, it finds them all (recall 1.0), but sometimes wrongly pulls in Legit ones (precision < 1).\n",
        "\n",
        "✅ Strong guarantee: If it predicts “Legit,” it’s really legit.\n",
        "\n",
        "⚠️ But if it predicts “Malicious,” there’s a ~12% chance it’s wrong."
      ],
      "metadata": {
        "id": "D3g2mT1Cx_Ug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ELMO Embeddings"
      ],
      "metadata": {
        "id": "Xwvx_BJXyy5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Training_df['Domains'] = Training_df['Domains'].apply(lambda x: re.sub(r'^www\\.', '', str(x).lower()))"
      ],
      "metadata": {
        "id": "rR4n3jIjx_G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = Training_df[\"Domains\"].astype(str).tolist()\n",
        "y_train = Training_df[\"Label\"].replace({\"__label__Legit\": 0, \"__label__Malicious\": 1}).values"
      ],
      "metadata": {
        "id": "sR_dWMP00_vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize_domain(domain):\n",
        "    # Split on non-alphanumeric characters and remove empty tokens\n",
        "    tokens = re.split(r'[^a-zA-Z0-9]', str(domain))\n",
        "    tokens = [t for t in tokens if t]\n",
        "    return \" \".join(tokens)\n"
      ],
      "metadata": {
        "id": "AF-L9jtMDLSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply tokenization\n",
        "Testing_df['Domain_tokens'] = Testing_df['Domains'].apply(tokenize_domain)\n",
        "\n",
        "Test_domains  = Testing_df['Domain_tokens'].tolist()"
      ],
      "metadata": {
        "id": "24BZv6_qUwQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")"
      ],
      "metadata": {
        "id": "nfnNlmqkUqZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_elmo_embeddings(sentences, batch_size=100): # Add batch_size parameter\n",
        "    embeddings = []\n",
        "    for i in range(0, len(sentences), batch_size): # Iterate in batches\n",
        "        batch_sentences = sentences[i:i + batch_size]\n",
        "        batch_embeddings = elmo.signatures['default'](tf.constant(batch_sentences))['elmo']\n",
        "        avg_embeddings = tf.reduce_mean(batch_embeddings, axis=1).numpy()\n",
        "        embeddings.append(avg_embeddings)\n",
        "    return np.vstack(embeddings) # Stack the batch embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "EAD-tmL8DuqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = get_elmo_embeddings(Test_domains, batch_size=512) # Process in batches of 512"
      ],
      "metadata": {
        "id": "wSZNm98yUn8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "clusters = kmeans.fit_predict(X_test)\n"
      ],
      "metadata": {
        "id": "QJ1mVd_hEyyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(Testing_df['Label'], clusters)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Legit','Malicious'],\n",
        "            yticklabels=['Legit','Malicious'])\n",
        "plt.xlabel(\"Predicted Cluster\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"KMeans on Tokenized ELMo Domain Embeddings\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9HHDh1JEF-rG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "X_test_2d = pca.fit_transform(X_test)\n"
      ],
      "metadata": {
        "id": "yCJbmXEyRwFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "plt.figure(figsize=(10,7))\n",
        "\n",
        "for label, color in [('Legit', 'green'), ('Malicious', 'red')]:\n",
        "    idx = [i for i, l in enumerate(clean_labels) if l == label]\n",
        "    plt.scatter(X_test_2d[idx,0], X_test_2d[idx,1], label=label, alpha=0.6, s=15, c=color)\n",
        "\n",
        "plt.title(\"FastText Domain Embeddings (PCA 2D)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wrq55m4IVNO2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.scatter(X_test_2d[:,0], X_test_2d[:,1], c=clusters, cmap='summer', alpha=0.6, s=15)\n",
        "plt.title(\"KMeans Clusters on Tokenized ELMo Domain Embeddings (PCA 2D)\")\n",
        "plt.colorbar(label=\"Cluster\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZJ-nUeH0Jk4Z",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(Testing_df['Label'], clusters))"
      ],
      "metadata": {
        "id": "s52KmIzUJZjH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers (Bert) Embeddings"
      ],
      "metadata": {
        "id": "atkJLvUHPSXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Train_df, Validation_df = train_test_split(Training_df[['Domains', 'Label']], test_size=0.2, random_state=42, stratify=Training_df['Label'])"
      ],
      "metadata": {
        "id": "TIkbebPFTZ-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "Ky9IrTVfTsUi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DomainDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=32)\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "# test_dataset = DomainDataset(test_domains, y_test)\n"
      ],
      "metadata": {
        "id": "Cc2KfBkAPXJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_domain(domain):\n",
        "    domain = str(domain).lower().strip()              # lowercase & remove spaces\n",
        "    domain = re.sub(r\"^www\\d*\\.\", \"\", domain)         # remove leading www / www2 etc.\n",
        "    domain = re.sub(r\"https?://\", \"\", domain)         # remove http/https\n",
        "    domain = re.sub(r\"[^a-z0-9\\.-]\", \"\", domain)      # keep only alphanumeric, dot, dash\n",
        "    domain = re.sub(r\"\\.{2,}\", \".\", domain)           # replace multiple dots with one\n",
        "    return domain\n"
      ],
      "metadata": {
        "id": "HJTz0823Vxg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train_df[\"Domains\"] = Train_df[\"Domains\"].apply(clean_domain)\n",
        "Train_df"
      ],
      "metadata": {
        "id": "KfYsIoXdV1Ur",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Validation_df[\"Domains\"] = Validation_df[\"Domains\"].apply(clean_domain)\n",
        "Validation_df"
      ],
      "metadata": {
        "id": "MD1iAEu4WMKr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_domain(domain):\n",
        "    tokens = re.split(r'[^a-z0-9]', domain)   # split on non-alphanumeric\n",
        "    tokens = [t for t in tokens if t]         # remove empty tokens\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "PEXG3PciV81z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train_df[\"Domains\"] = Train_df[\"Domains\"].apply(tokenize_domain)\n",
        "Train_df"
      ],
      "metadata": {
        "id": "J6BBQx8hWC2V",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Validation_df[\"Domains\"] = Validation_df[\"Domains\"].apply(tokenize_domain)\n",
        "Validation_df"
      ],
      "metadata": {
        "id": "CPkRSARWWEr5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\"__label__Legit\": 0, \"__label__Malicious\":1}\n",
        "Train_df[\"Label\"] = Train_df[\"Label\"].map(label_map)\n",
        "Train_df"
      ],
      "metadata": {
        "id": "SU4kGP6kaEKu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Validation_df[\"Label\"] = Validation_df[\"Label\"].map(label_map)\n",
        "Validation_df"
      ],
      "metadata": {
        "id": "krbfbIJXaOIP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DomainDataset(Train_df[\"Domains\"].tolist(), Train_df[\"Label\"].tolist())"
      ],
      "metadata": {
        "id": "zphAfsZyTxW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset  = DomainDataset(Validation_df[\"Domains\"].tolist(), Validation_df[\"Label\"].tolist())"
      ],
      "metadata": {
        "id": "LyF4CYyabP74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }"
      ],
      "metadata": {
        "id": "sVuCU4qrmT-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        ")"
      ],
      "metadata": {
        "id": "7iIAVVXNQI4U",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "gw4C9Io_XXTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "09ZtWg4gWirD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace path with your saved checkpoint\n",
        "MODEL_PATH = \"./results/checkpoint-best\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "base_model = AutoModel.from_pretrained(MODEL_PATH)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "RFssGCvKQN5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_Bert_embeddings(texts, model):\n",
        "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=32, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encodings)\n",
        "    # Mean pooling across tokens\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "    return embeddings\n"
      ],
      "metadata": {
        "id": "sAHygI_Q6UdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = get_Bert_embeddings(Testing_df['Domains'].tolist(), base_model)\n",
        "X_test"
      ],
      "metadata": {
        "id": "h3nTeRxWXQV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = Testing_df['Label'].map(label_map).values\n",
        "y_test"
      ],
      "metadata": {
        "id": "wFiLOBFE7u_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KMeans clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "clusters = kmeans.fit_predict(X_test)\n"
      ],
      "metadata": {
        "id": "mmqn0lLcQSKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA 2D\n",
        "pca = PCA(n_components=2)\n",
        "PCA_Embeddings = pca.fit_transform(X_test)"
      ],
      "metadata": {
        "id": "rSja6U-YAWM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "\n",
        "for label, color in [('Legit', 'green'), ('Malicious', 'red')]:\n",
        "    idx = [i for i, l in enumerate(clean_labels) if l == label]\n",
        "    plt.scatter(PCA_Embeddings[idx,0], PCA_Embeddings[idx,1], label=label, alpha=0.6, s=15, c=color)\n",
        "\n",
        "plt.title(\"FastText Domain Embeddings (PCA 2D)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TQqKT2mpALn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "plt.scatter(PCA_Embeddings[:,0], PCA_Embeddings[:,1], c=clusters, cmap=\"coolwarm\", alpha=0.6, s=15)\n",
        "plt.title(\"Transformer Embeddings of Domains (PCA 2D)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EOJogYbJ8bZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, clusters)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Legit\",\"Malicious\"], yticklabels=[\"Legit\",\"Malicious\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(y_test, clusters))\n"
      ],
      "metadata": {
        "id": "mN3Utf0AQW2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variational AutoEncoder"
      ],
      "metadata": {
        "id": "-piC1DDWIY1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What makes VAEs special is their ability to model data probabilistically, which means they don’t just learn compressed versions of data but learn to generate new data points from the latent distribution. [Latent Space Representation](https://medium.com/@whyamit101/latent-space-representations-in-variational-autoencoders-vaes-e74076eda77b)"
      ],
      "metadata": {
        "id": "OanQNFaMJTpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "domains = Training_df[\"Domains\"].astype(str).tolist()"
      ],
      "metadata": {
        "id": "CmVUzYTgE0HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ngrams(domain, n_sizes=[2,3,4]):\n",
        "    domain = domain.lower().replace('.', '')\n",
        "    ngrams = []\n",
        "    for n in n_sizes:\n",
        "        ngrams += [domain[i:i+n] for i in range(len(domain)-n+1)]\n",
        "    return ngrams"
      ],
      "metadata": {
        "id": "cx9MNdfF_6cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ngrams = []\n",
        "for domain in domains:\n",
        "    all_ngrams += generate_ngrams(domain, n_sizes=[2,3,4])\n",
        "\n",
        "ngram_counts = Counter(all_ngrams)\n",
        "most_common_ngrams = [ngram for ngram, _ in ngram_counts.most_common(vocab_size)]\n",
        "ngram2idx = {ng: i+1 for i, ng in enumerate(most_common_ngrams)}"
      ],
      "metadata": {
        "id": "_WZbU8tpFLfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_domain_ngrams(domain, ngram2idx, n_sizes=[2,3,4], max_len=20):\n",
        "    ngrams = generate_ngrams(domain, n_sizes)\n",
        "    seq = [ngram2idx.get(ng, 0) for ng in ngrams]\n",
        "    if len(seq) < max_len:\n",
        "        seq += [0]*(max_len-len(seq))\n",
        "    else:\n",
        "        seq = seq[:max_len]\n",
        "    return seq"
      ],
      "metadata": {
        "id": "RgVD0hH6FOMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_domains = np.array([encode_domain_ngrams(d, ngram2idx, max_len=max_len) for d in domains])"
      ],
      "metadata": {
        "id": "6yrAjXGr_-4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 20\n",
        "vocab_size = len()\n",
        "embed_dim = 32\n",
        "latent_dim = 64"
      ],
      "metadata": {
        "id": "eMLagn-SUiIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_domains"
      ],
      "metadata": {
        "id": "_Bki3ufcF_4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### -------------------------------\n",
        "### Sampling layer and KL Divergence layer\n",
        "### -------------------------------"
      ],
      "metadata": {
        "id": "0-1Ag0QeU5S-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Sampling(Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "\n",
        "class KLLossLayer(Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        kl = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "        kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(kl, axis=1))\n",
        "        self.add_loss(kl_loss)\n",
        "        return inputs\n"
      ],
      "metadata": {
        "id": "ADqX2W3e9D7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VAE Model"
      ],
      "metadata": {
        "id": "Z06M9cmEVTAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, Model, Input"
      ],
      "metadata": {
        "id": "8uZeSe9BXSyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=(max_len,), name=\"encoder_input\")\n",
        "x = Embedding(input_dim=vocab_size+1, output_dim=embed_dim, input_length=max_len)(inputs)\n",
        "x = Conv1D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = MaxPooling1D(2, padding=\"same\")(x)\n",
        "\n",
        "x = Conv1D(32, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "x = MaxPooling1D(2, padding=\"same\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "\n",
        "x = LSTM(64)(x)\n",
        "x = layers.LayerNormalization()(x)\n",
        "\n",
        "z_mean = Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "\n",
        "# KL Loss Layer\n",
        "z_mean, z_log_var = KLLossLayer()([z_mean, z_log_var])\n",
        "\n",
        "# Latent space sampling\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "# Decoder\n",
        "x = RepeatVector(max_len)(z)\n",
        "x = LSTM(32, return_sequences=True)(x)\n",
        "x = layers.LayerNormalization()(x)\n",
        "\n",
        "decoded = Conv1D(vocab_size+1, 3, activation=\"softmax\", padding=\"same\", name=\"decoder_output\")(x)\n",
        "\n",
        "vae = Model(inputs, decoded, name=\"vae\")\n"
      ],
      "metadata": {
        "id": "G-S4lgUm9k6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruction_loss(y_true, y_pred):\n",
        "    recon = sparse_categorical_crossentropy(y_true, y_pred)  # (batch, max_len)\n",
        "    return tf.reduce_mean(tf.reduce_sum(recon, axis=1))\n",
        "\n",
        "vae.compile(optimizer=\"adam\", loss=reconstruction_loss)\n",
        "vae.summary()\n"
      ],
      "metadata": {
        "id": "NrWY1f3t_4mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define checkpoint callback\n",
        "checkpoint_cb = ModelCheckpoint(\n",
        "    filepath=\"/content/drive/MyDrive/MSC Work/RQ1/FastText_Model/vae_best_model.keras\",   # Save best model\n",
        "    monitor=\"loss\",                    # Monitor training loss (or 'val_loss' if using validation data)\n",
        "    save_best_only=True,               # Only save when model improves\n",
        "    save_weights_only=False,           # Save full model (architecture + weights + optimizer)\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "ReVVv-a0Fz_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train with checkpoint\n",
        "history = vae.fit(\n",
        "    encoded_domains, encoded_domains,\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    callbacks=[checkpoint_cb]\n",
        ")"
      ],
      "metadata": {
        "id": "DqojQrx4GMJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "if 'val_loss' in history.history:   # if you added validation data\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('VAE Training Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "e9R08-1gGnnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Character vocabulary\n",
        "chars = list(string.ascii_lowercase + string.digits + \".\")\n"
      ],
      "metadata": {
        "id": "UxaXrw7NrutY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars"
      ],
      "metadata": {
        "id": "P7Do4bVHr7hK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = ['<PAD>', '<UNK>'] + chars  # include padding and unknown\n",
        "print(chars)\n",
        "char2idx = {c: i for i, c in enumerate(chars)}\n",
        "idx2char = {i: c for c, i in char2idx.items()}"
      ],
      "metadata": {
        "id": "fyFEWm-Vrx_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char2idx"
      ],
      "metadata": {
        "id": "U15Syb5Cr3We",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx2char"
      ],
      "metadata": {
        "id": "ovB0uthYr4wx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised"
      ],
      "metadata": {
        "id": "rdG0P4BP8DXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save as plain text (one domain per line)\n",
        "train_df['Legitimate Domain'].to_csv(\"/content/drive/MyDrive/MSC Work/RQ1/Dataset/Domains/domains_train.txt\", index=False, header=False)\n",
        "test_df['Legitimate Domain'].to_csv(\"/content/drive/MyDrive/MSC Work/RQ1/Dataset/Domains/domains_test.txt\", index=False, header=False)"
      ],
      "metadata": {
        "id": "-HGZn-AKqzVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.train_unsupervised(\n",
        "    input=\"/content/drive/MyDrive/MSC Work/RQ1/Dataset/Domains/domains_train.txt\",\n",
        "    model=\"skipgram\",    # CBOW is also possible\n",
        "    dim=100,             # embedding size\n",
        "    minn=2, maxn=5       # use character n-grams (important for short text)\n",
        ")\n",
        "\n",
        "model.save_model(\"/content/drive/MyDrive/MSC Work/RQ1/FastText_Model/domain_embeddings.bin\")"
      ],
      "metadata": {
        "id": "R7QmFip50ef9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test domains\n",
        "with open(\"/content/drive/MyDrive/MSC Work/RQ1/Dataset/Domains/domains_test.txt\") as f:\n",
        "    test_domains = [line.strip() for line in f]\n",
        "\n",
        "# Get embeddings\n",
        "embeddings = np.array([model.get_word_vector(d) for d in test_domains])"
      ],
      "metadata": {
        "id": "6Fxu-mDA1VFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase\n",
        "Illegitimate_Domains_df['Domains'] = Illegitimate_Domains_df['Domains'].str.lower()\n",
        "\n",
        "# Split\n",
        "train2_df, test2_df = train_test_split(Illegitimate_Domains_df, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "3cPmKHcB6uL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save as plain text (one domain per line)\n",
        "train2_df['Domains'].to_csv(\"/content/drive/MyDrive/MSC Work/RQ1/Dataset/Domains/domains2_train.txt\", index=False, header=False)\n",
        "test2_df['Domains'].to_csv(\"/content/drive/MyDrive/MSC Work/RQ1/Dataset/Domains/domains2_test.txt\", index=False, header=False)"
      ],
      "metadata": {
        "id": "69w-oIMJqgNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.train_unsupervised(\n",
        "    input=\"/content/drive/MyDrive/MSC Work/RQ1/Dataset/Domains/domains2_train.txt\",\n",
        "    model=\"skipgram\",    # CBOW is also possible\n",
        "    dim=100,             # embedding size\n",
        "    minn=2, maxn=5       # use character n-grams (important for short text)\n",
        ")\n",
        "\n",
        "model.save_model(\"/content/drive/MyDrive/MSC Work/RQ1/FastText_Model/domain2_embeddings.bin\")"
      ],
      "metadata": {
        "id": "n2Gfefpz6puX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test domains\n",
        "with open(\"/content/drive/MyDrive/MSC Work/RQ1/Dataset/Domains/domains2_test.txt\") as f:\n",
        "    test2_domains = [line.strip() for line in f]\n",
        "\n",
        "# Get embeddings\n",
        "embeddings2 = np.array([model.get_word_vector(d) for d in test2_domains])"
      ],
      "metadata": {
        "id": "mmz8Qxza72LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce to 2D\n",
        "pca = PCA(n_components=2)\n",
        "reduced = pca.fit_transform(embeddings2)\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.scatter(reduced[:,0], reduced[:,1])\n",
        "\n",
        "for i, domain in enumerate(test_domains):\n",
        "    plt.annotate(domain, (reduced[i,0], reduced[i,1]), fontsize=8, alpha=0.7)\n",
        "\n",
        "plt.title(\"FastText Domain Embeddings (PCA 2D)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KcGfSwvQ7r0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Reduce to 2D\n",
        "pca = PCA(n_components=2)\n",
        "reduced = pca.fit_transform(embeddings)\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.scatter(reduced[:,0], reduced[:,1])\n",
        "\n",
        "for i, domain in enumerate(test_domains):\n",
        "    plt.annotate(domain, (reduced[i,0], reduced[i,1]), fontsize=8, alpha=0.7)\n",
        "\n",
        "plt.title(\"FastText Domain Embeddings (PCA 2D)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "80NPETY31uCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u-tennhLqCnu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}